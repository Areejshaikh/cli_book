---
sidebar_position: 4
---

# Vision-Language-Action (VLA)

## Module Overview

This module covers integrating LLMs with robotics, focusing on voice-to-action capabilities using OpenAI Whisper and cognitive planning with LLMs that convert natural language to ROS 2 actions.

## Learning Outcomes

By the end of this module, you will:
- Understand how to implement voice-to-action using OpenAI Whisper
- Know how to create cognitive planning systems that convert natural language to ROS 2 actions
- Be able to implement a capstone project where an autonomous humanoid executes voice commands, navigates, identifies, and manipulates objects